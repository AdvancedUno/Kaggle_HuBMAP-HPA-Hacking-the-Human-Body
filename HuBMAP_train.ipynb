{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":481607,"status":"ok","timestamp":1661735612753,"user":{"displayName":"qwe qwe","userId":"08905623062927154995"},"user_tz":-540},"id":"U8b6QMRAg_4B","outputId":"507721ea-d088-4186-bb55-a79d8873c671"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n","Collecting torch==1.12.0\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1837.7 MB)\n","\u001b[K     |██████████████▌                 | 834.1 MB 1.3 MB/s eta 0:13:16tcmalloc: large alloc 1147494400 bytes == 0x3a784000 @  0x7f5b1c3dd615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |██████████████████▍             | 1055.7 MB 1.3 MB/s eta 0:10:25tcmalloc: large alloc 1434370048 bytes == 0x7edda000 @  0x7f5b1c3dd615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |███████████████████████▎        | 1336.2 MB 1.3 MB/s eta 0:06:29tcmalloc: large alloc 1792966656 bytes == 0x3c0c000 @  0x7f5b1c3dd615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |█████████████████████████████▌  | 1691.1 MB 1.3 MB/s eta 0:01:56tcmalloc: large alloc 2241208320 bytes == 0x6e9f4000 @  0x7f5b1c3dd615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 1837.6 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1837662208 bytes == 0xf4356000 @  0x7f5b1c3dc1e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n","tcmalloc: large alloc 2297077760 bytes == 0x161bde000 @  0x7f5b1c3dd615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n","\u001b[K     |████████████████████████████████| 1837.7 MB 8.2 kB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Collecting torchvision\n","  Downloading torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[K     |████████████████████████████████| 19.1 MB 1.1 MB/s \n","\u001b[?25h  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (23.4 MB)\n","\u001b[K     |████████████████████████████████| 23.4 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.13.1+cu113\n","    Uninstalling torchvision-0.13.1+cu113:\n","      Successfully uninstalled torchvision-0.13.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.12.0+cu113 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.12.0+cu113 which is incompatible.\u001b[0m\n","Successfully installed torch-1.12.0+cu113 torchvision-0.13.0+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openmim\n","  Downloading openmim-0.2.1-py2.py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 5.0 MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from openmim) (1.3.5)\n","Collecting model-index\n","  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n","Collecting rich\n","  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 49.7 MB/s \n","\u001b[?25hRequirement already satisfied: pip>=19.3 in /usr/local/lib/python3.7/dist-packages (from openmim) (21.1.3)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from openmim) (0.8.10)\n","Requirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from openmim) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openmim) (2.23.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from model-index->openmim) (6.0)\n","Collecting ordered-set\n","  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from model-index->openmim) (3.4.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown->model-index->openmim) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (4.1.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->openmim) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (1.24.3)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->openmim) (2.6.1)\n","Installing collected packages: ordered-set, commonmark, rich, model-index, colorama, openmim\n","Successfully installed colorama-0.4.5 commonmark-0.9.1 model-index-0.1.11 openmim-0.2.1 ordered-set-4.1.0 rich-12.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu113/torch1.12.0/index.html\n","Collecting mmcv-full==1.6.0\n","  Downloading https://download.openmmlab.com/mmcv/dist/cu113/torch1.12.0/mmcv_full-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (40.1 MB)\n","\u001b[K     |████████████████████████████████| 40.1 MB 201 kB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (1.21.6)\n","Collecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Collecting yapf\n","  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n","\u001b[K     |████████████████████████████████| 190 kB 31.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (21.3)\n","Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (4.6.0.66)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.6.0) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full==1.6.0) (3.0.9)\n","Installing collected packages: yapf, addict, mmcv-full\n","Successfully installed addict-2.4.0 mmcv-full-1.6.0 yapf-0.32.0\n","Cloning into 'mmsegmentation'...\n","remote: Enumerating objects: 11204, done.\u001b[K\n","remote: Counting objects: 100% (1079/1079), done.\u001b[K\n","remote: Compressing objects: 100% (555/555), done.\u001b[K\n","remote: Total 11204 (delta 616), reused 835 (delta 512), pack-reused 10125\u001b[K\n","Receiving objects: 100% (11204/11204), 14.92 MiB | 38.00 MiB/s, done.\n","Resolving deltas: 100% (8046/8046), done.\n","/content/mmsegmentation\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/mmsegmentation\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (3.2.2)\n","Collecting mmcls>=0.20.1\n","  Downloading mmcls-0.23.2-py2.py3-none-any.whl (578 kB)\n","\u001b[K     |████████████████████████████████| 578 kB 25.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (21.3)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from mmsegmentation==0.27.0) (3.3.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation==0.27.0) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mmsegmentation==0.27.0) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mmsegmentation==0.27.0) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->mmsegmentation==0.27.0) (4.12.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->mmsegmentation==0.27.0) (0.2.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->mmsegmentation==0.27.0) (3.8.1)\n","Installing collected packages: mmcls, mmsegmentation\n","  Running setup.py develop for mmsegmentation\n","Successfully installed mmcls-0.23.2 mmsegmentation-0.27.0\n","Setup complete. Using torch 1.12.0+cu113 (Tesla T4)\n","Mounted at /content/drive\n"]}],"source":["!pip install torch==1.12.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n","# Install MMCV\n","!pip install openmim\n","!mim install mmcv-full==1.6.0\n","!git clone https://github.com/open-mmlab/mmsegmentation.git\n","\n","%cd mmsegmentation\n","!pip install -e .\n","\n","import torch\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n","\n","from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n","import mmcv\n","import os.path as osp\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install spams\n","!pip install staintools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86vQf4ipLY4K","executionInfo":{"status":"ok","timestamp":1661736550038,"user_tz":-540,"elapsed":65019,"user":{"displayName":"qwe qwe","userId":"08905623062927154995"}},"outputId":"2802c0fb-1665-450f-d2d7-405f05d89301"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spams\n","  Downloading spams-2.6.5.4.tar.gz (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 24.3 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from spams) (1.7.3)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from spams) (1.21.6)\n","Requirement already satisfied: Pillow>=6.0 in /usr/local/lib/python3.7/dist-packages (from spams) (7.1.2)\n","Building wheels for collected packages: spams\n","  Building wheel for spams (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spams: filename=spams-2.6.5.4-cp37-cp37m-linux_x86_64.whl size=3236588 sha256=744f38ddc0d465d06f01ee2c4eba1b082bdadf75cbcf2235878966f561a597c6\n","  Stored in directory: /root/.cache/pip/wheels/5e/92/17/e718e0e26bfcdd7e81afcf874ac2bc599dd7dd7f3ad78f9d76\n","Successfully built spams\n","Installing collected packages: spams\n","Successfully installed spams-2.6.5.4\n"]}]},{"cell_type":"code","source":["%cd /content/mmsegmentation/mmseg/datasets/pipelines"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEBMAT3yPM9x","executionInfo":{"status":"ok","timestamp":1661736917513,"user_tz":-540,"elapsed":5,"user":{"displayName":"qwe qwe","userId":"08905623062927154995"}},"outputId":"dade125f-8795-4728-afff-1041f088e513"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/mmsegmentation/mmseg/datasets/pipelines\n"]}]},{"cell_type":"code","source":["%%bash\n","\n","cat <<EOT >> ./pixelNthick.py\n","from ..builder import PIPELINES\n","\n","@PIPELINES.register_module()\n","class MyTransform:\n","\n","    def __call__(self, image, domain_pixel_size=0.4, target_pixel_size=0.4, domain_tissue_thickness=4, target_tissue_thickness=4, alpha=0.15):  \n","      # Augment tissue thickness\n","      tissue_thickness_scale_factor = target_tissue_thickness - domain_tissue_thickness\n","      image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n","      image_hsv[:, :, 1] *= (1 + (alpha * tissue_thickness_scale_factor))\n","      image_hsv[:, :, 2] *= (1 - (alpha * tissue_thickness_scale_factor))\n","      image_hsv = image_hsv.astype(np.uint8)\n","      image_scaled = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2RGB)\n","      \n","      # Standardize luminosity\n","      image_scaled = staintools.LuminosityStandardizer.standardize(image_scaled)\n","\n","      # Augment pixel size\n","      pixel_size_scale_factor = domain_pixel_size / target_pixel_size\n","      image_resized = cv2.resize(\n","          image_scaled,\n","          dsize=None,\n","          fx=pixel_size_scale_factor,\n","          fy=pixel_size_scale_factor,\n","          interpolation=cv2.INTER_CUBIC\n","      )\n","      image_resized = cv2.resize(\n","          image_resized,\n","          dsize=(\n","              image.shape[1],\n","              image.shape[0]\n","          ),\n","          interpolation=cv2.INTER_CUBIC\n","      )\n","      \n","      # Standardize luminosity\n","      image = staintools.LuminosityStandardizer.standardize(image)\n","      image_augmented = staintools.LuminosityStandardizer.standardize(image_resized)\n","      \n","      return (np.array(image_augmented).astype(np.float32))/255\n","\n","EOT"],"metadata":{"id":"noaqECQtBQaz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from glob import glob\n","import numpy as np\n","import cv2\n","import os\n","from sklearn.model_selection import StratifiedKFold\n","\n","Fold = 10\n","all_mask_files = glob(\"/content/drive/MyDrive/hubmap-organ-segmentation/masks/*\")\n","masks = []\n","num_mask = np.zeros((6,Fold))\n","\n","!mkdir splits\n","\n","split = list(StratifiedKFold(n_splits=Fold, shuffle=True, random_state=2022).split(all_mask_files, [0]*len(all_mask_files)))\n","for fold, (train_idx, valid_idx) in enumerate(split):\n","    with open(f\"/content/drive/MyDrive/hubmap-organ-segmentation/splits/fold_{fold}.txt\", \"w\") as f:\n","        for idx in train_idx:\n","            f.write(os.path.basename(all_mask_files[idx])[:-4] + \"\\n\")\n","    with open(f\"/content/drive/MyDrive/hubmap-organ-segmentation/splits/valid_{fold}.txt\", \"w\") as f:\n","        for idx in valid_idx:\n","            f.write(os.path.basename(all_mask_files[idx])[:-4] + \"\\n\")"],"metadata":{"id":"puyJzoIFmYKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd "],"metadata":{"id":"wqV8qG4oNuNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","\n","cat <<EOT >> ./config2.py\n","\n","_base_ = ['/content/mmsegmentation/configs/segformer/segformer_mit-b0_512x512_160k_ade20k.py']\n","\n","# dataset settings\n","dataset_type = 'CustomDataset'\n","data_root = '/content/drive/MyDrive/hubmap-organ-segmentation/'\n","classes = ['background', 'kidney', 'prostate', 'largeintestine', 'spleen', 'lung']\n","palette = [[0,0,0], [255,0,0], [0,255,0], [0,0,255], [255,255,0], [255,0,255]]\n","\n","# img_norm_cfg = dict(mean=[196.869, 190.186, 194.802], std=[63.010, 66.765, 65.745], to_rgb=True)\n","img_norm_cfg = dict(mean=[0.77907741, 0.75481993, 0.77184585], std=[0.25398408, 0.26743021, 0.26288133])\n","size = 256\n","\n","train_pipeline = [\n","    dict(type='LoadImageFromFile', to_float32=True),\n","    dict(type='LoadAnnotations'),\n","    dict(type='Resize', img_scale=(size, size), keep_ratio=True),\n","    dict(type='MyTransform'),\n","    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n","    dict(type='Normalize', **img_norm_cfg),\n","    dict(type='Pad', size=(size, size), pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile', to_float32=True),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(size, size),\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(type='Normalize', **img_norm_cfg),\n","            dict(type='Pad', size=(size, size), pad_val=0, seg_pad_val=255),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img']),\n","        ])\n","]\n","data = dict(\n","    samples_per_gpu=8,\n","    workers_per_gpu=4,\n","    train=dict(\n","        type=dataset_type,\n","        data_root=data_root,\n","        img_dir='train',\n","        ann_dir='masks',\n","        img_suffix=\".png\",\n","        seg_map_suffix='.png',\n","        split=\"splits/fold_0.txt\",\n","        classes=classes,\n","        palette=palette,\n","        pipeline=train_pipeline),\n","    val=dict(\n","        type=dataset_type,\n","        data_root=data_root,\n","        img_dir='train',\n","        ann_dir='masks',\n","        img_suffix=\".png\",\n","        seg_map_suffix='.png',\n","        split=\"splits/valid_0.txt\",\n","        classes=classes,\n","        palette=palette,\n","        pipeline=test_pipeline),\n","    test=dict(\n","        type=dataset_type,\n","        data_root=data_root,\n","        test_mode=True,\n","        img_dir='train',\n","        ann_dir='masks',\n","        img_suffix=\".png\",\n","        seg_map_suffix='.png',\n","        classes=classes,\n","        palette=palette,\n","        pipeline=test_pipeline))\n","\n","checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth'  # noqa\n","\n","# model settings\n","model = dict(\n","    pretrained=checkpoint,\n","    backbone=dict(\n","        embed_dims=64, num_heads=[1, 2, 5, 8], num_layers=[3, 6, 40, 3]),\n","    decode_head=dict(in_channels=[64, 128, 320, 512]))\n","\n","EOT"],"metadata":{"id":"W2PTK0LUe4n8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python ./tools/train.py ./config2.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MnjUHohli5H","outputId":"4575207d-d68e-4cd2-c515-674d6d2137eb","executionInfo":{"status":"ok","timestamp":1661737935643,"user_tz":-540,"elapsed":32244,"user":{"displayName":"qwe qwe","userId":"08905623062927154995"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-08-29 01:49:42,756 - mmseg - INFO - Multi-processing start method is `None`\n","2022-08-29 01:49:42,756 - mmseg - INFO - OpenCV num_threads is `2\n","2022-08-29 01:49:42,793 - mmseg - INFO - Environment info:\n","------------------------------------------------------------\n","sys.platform: linux\n","Python: 3.7.13 (default, Apr 24 2022, 01:04:09) [GCC 7.5.0]\n","CUDA available: True\n","GPU 0: Tesla T4\n","CUDA_HOME: /usr/local/cuda\n","NVCC: Cuda compilation tools, release 11.1, V11.1.105\n","GCC: x86_64-linux-gnu-gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n","PyTorch: 1.12.0+cu113\n","PyTorch compiling details: PyTorch built with:\n","  - GCC 9.3\n","  - C++ Version: 201402\n","  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n","  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n","  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n","  - LAPACK is enabled (usually provided by MKL)\n","  - NNPACK is enabled\n","  - CPU capability usage: AVX2\n","  - CUDA Runtime 11.3\n","  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n","  - CuDNN 8.3.2  (built against CUDA 11.5)\n","  - Magma 2.5.2\n","  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n","\n","TorchVision: 0.13.0+cu113\n","OpenCV: 4.6.0\n","MMCV: 1.6.0\n","MMCV Compiler: GCC 9.3\n","MMCV CUDA Compiler: 11.3\n","MMSegmentation: 0.27.0+7a0f45e\n","------------------------------------------------------------\n","\n","2022-08-29 01:49:42,793 - mmseg - INFO - Distributed training: False\n","2022-08-29 01:49:43,699 - mmseg - INFO - Config:\n","norm_cfg = dict(type='SyncBN', requires_grad=True)\n","model = dict(\n","    type='EncoderDecoder',\n","    pretrained=\n","    'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth',\n","    backbone=dict(\n","        type='MixVisionTransformer',\n","        in_channels=3,\n","        embed_dims=64,\n","        num_stages=4,\n","        num_layers=[3, 6, 40, 3],\n","        num_heads=[1, 2, 5, 8],\n","        patch_sizes=[7, 3, 3, 3],\n","        sr_ratios=[8, 4, 2, 1],\n","        out_indices=(0, 1, 2, 3),\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        drop_rate=0.0,\n","        attn_drop_rate=0.0,\n","        drop_path_rate=0.1),\n","    decode_head=dict(\n","        type='SegformerHead',\n","        in_channels=[64, 128, 320, 512],\n","        in_index=[0, 1, 2, 3],\n","        channels=256,\n","        dropout_ratio=0.1,\n","        num_classes=150,\n","        norm_cfg=dict(type='SyncBN', requires_grad=True),\n","        align_corners=False,\n","        loss_decode=dict(\n","            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n","    train_cfg=dict(),\n","    test_cfg=dict(mode='whole'))\n","dataset_type = 'CustomDataset'\n","data_root = '/content/drive/MyDrive/hubmap-organ-segmentation/'\n","img_norm_cfg = dict(\n","    mean=[196.869, 190.186, 194.802], std=[63.01, 66.765, 65.745], to_rgb=True)\n","crop_size = (512, 512)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations'),\n","    dict(type='Resize', img_scale=(256, 256), keep_ratio=True),\n","    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n","    dict(\n","        type='Normalize',\n","        mean=[196.869, 190.186, 194.802],\n","        std=[63.01, 66.765, 65.745],\n","        to_rgb=True),\n","    dict(type='Pad', size=(256, 256), pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n","    dict(type='MyTransform')\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(256, 256),\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(\n","                type='Normalize',\n","                mean=[196.869, 190.186, 194.802],\n","                std=[63.01, 66.765, 65.745],\n","                to_rgb=True),\n","            dict(type='Pad', size=(256, 256), pad_val=0, seg_pad_val=255),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]\n","data = dict(\n","    samples_per_gpu=8,\n","    workers_per_gpu=4,\n","    train=dict(\n","        type='CustomDataset',\n","        data_root='/content/drive/MyDrive/hubmap-organ-segmentation/',\n","        img_dir='train',\n","        ann_dir='masks',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(type='LoadAnnotations'),\n","            dict(type='Resize', img_scale=(256, 256), keep_ratio=True),\n","            dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n","            dict(\n","                type='Normalize',\n","                mean=[196.869, 190.186, 194.802],\n","                std=[63.01, 66.765, 65.745],\n","                to_rgb=True),\n","            dict(type='Pad', size=(256, 256), pad_val=0, seg_pad_val=255),\n","            dict(type='DefaultFormatBundle'),\n","            dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n","            dict(type='MyTransform')\n","        ],\n","        img_suffix='.png',\n","        seg_map_suffix='.png',\n","        split='splits/fold_0.txt',\n","        classes=[\n","            'background', 'kidney', 'prostate', 'largeintestine', 'spleen',\n","            'lung'\n","        ],\n","        palette=[[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255],\n","                 [255, 255, 0], [255, 0, 255]]),\n","    val=dict(\n","        type='CustomDataset',\n","        data_root='/content/drive/MyDrive/hubmap-organ-segmentation/',\n","        img_dir='train',\n","        ann_dir='masks',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(256, 256),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[196.869, 190.186, 194.802],\n","                        std=[63.01, 66.765, 65.745],\n","                        to_rgb=True),\n","                    dict(\n","                        type='Pad',\n","                        size=(256, 256),\n","                        pad_val=0,\n","                        seg_pad_val=255),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ],\n","        img_suffix='.png',\n","        seg_map_suffix='.png',\n","        split='splits/valid_0.txt',\n","        classes=[\n","            'background', 'kidney', 'prostate', 'largeintestine', 'spleen',\n","            'lung'\n","        ],\n","        palette=[[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255],\n","                 [255, 255, 0], [255, 0, 255]]),\n","    test=dict(\n","        type='CustomDataset',\n","        data_root='/content/drive/MyDrive/hubmap-organ-segmentation/',\n","        img_dir='train',\n","        ann_dir='masks',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(256, 256),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[196.869, 190.186, 194.802],\n","                        std=[63.01, 66.765, 65.745],\n","                        to_rgb=True),\n","                    dict(\n","                        type='Pad',\n","                        size=(256, 256),\n","                        pad_val=0,\n","                        seg_pad_val=255),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ],\n","        test_mode=True,\n","        img_suffix='.png',\n","        seg_map_suffix='.png',\n","        classes=[\n","            'background', 'kidney', 'prostate', 'largeintestine', 'spleen',\n","            'lung'\n","        ],\n","        palette=[[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255],\n","                 [255, 255, 0], [255, 0, 255]]))\n","log_config = dict(\n","    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n","dist_params = dict(backend='nccl')\n","log_level = 'INFO'\n","load_from = None\n","resume_from = None\n","workflow = [('train', 1)]\n","cudnn_benchmark = True\n","optimizer = dict(\n","    type='AdamW',\n","    lr=6e-05,\n","    betas=(0.9, 0.999),\n","    weight_decay=0.01,\n","    paramwise_cfg=dict(\n","        custom_keys=dict(\n","            pos_block=dict(decay_mult=0.0),\n","            norm=dict(decay_mult=0.0),\n","            head=dict(lr_mult=10.0))))\n","optimizer_config = dict()\n","lr_config = dict(\n","    policy='poly',\n","    warmup='linear',\n","    warmup_iters=1500,\n","    warmup_ratio=1e-06,\n","    power=1.0,\n","    min_lr=0.0,\n","    by_epoch=False)\n","runner = dict(type='IterBasedRunner', max_iters=160000)\n","checkpoint_config = dict(by_epoch=False, interval=16000)\n","evaluation = dict(interval=16000, metric='mIoU', pre_eval=True)\n","checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth'\n","classes = [\n","    'background', 'kidney', 'prostate', 'largeintestine', 'spleen', 'lung'\n","]\n","palette = [[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0],\n","           [255, 0, 255]]\n","size = 256\n","work_dir = './work_dirs/config2'\n","gpu_ids = [0]\n","auto_resume = False\n","\n","2022-08-29 01:49:43,699 - mmseg - INFO - Set random seed to 1017531004, deterministic: False\n","/content/mmsegmentation/mmseg/models/backbones/mit.py:365: UserWarning: DeprecationWarning: pretrained is deprecated, please use \"init_cfg\" instead\n","  warnings.warn('DeprecationWarning: pretrained is deprecated, '\n","/content/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n","  'Default ``avg_non_ignore`` is False, if you would like to '\n","2022-08-29 01:49:44,496 - mmseg - INFO - initialize MixVisionTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth'}\n","2022-08-29 01:49:44,497 - mmcv - INFO - load model from: https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth\n","2022-08-29 01:49:44,497 - mmcv - INFO - load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth\n","2022-08-29 01:49:45,048 - mmseg - INFO - initialize SegformerHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n","./tools/train.py:207: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.\n","  'SyncBN is only supported with DDP. To be compatible with DP, '\n","2022-08-29 01:49:45,065 - mmseg - INFO - EncoderDecoder(\n","  (backbone): MixVisionTransformer(\n","    (layers): ModuleList(\n","      (0): ModuleList(\n","        (0): PatchEmbed(\n","          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n","          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): ModuleList(\n","          (0): TransformerEncoderLayer(\n","            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n","              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (1): TransformerEncoderLayer(\n","            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n","              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (2): TransformerEncoderLayer(\n","            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n","              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","        )\n","        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (1): ModuleList(\n","        (0): PatchEmbed(\n","          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): ModuleList(\n","          (0): TransformerEncoderLayer(\n","            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n","              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (1): TransformerEncoderLayer(\n","            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n","              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (2): TransformerEncoderLayer(\n","            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n","              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (3): TransformerEncoderLayer(\n","            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n","              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (4): TransformerEncoderLayer(\n","            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n","              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (5): TransformerEncoderLayer(\n","            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n","              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","        )\n","        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (2): ModuleList(\n","        (0): PatchEmbed(\n","          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): ModuleList(\n","          (0): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (1): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (2): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (3): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (4): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (5): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (6): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (7): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (8): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (9): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (10): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (11): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (12): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (13): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (14): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (15): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (16): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (17): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (18): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (19): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (20): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (21): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (22): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (23): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (24): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (25): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (26): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (27): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (28): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (29): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (30): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (31): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (32): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (33): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (34): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (35): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (36): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (37): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (38): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (39): TransformerEncoderLayer(\n","            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n","              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            )\n","            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","        )\n","        (2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (3): ModuleList(\n","        (0): PatchEmbed(\n","          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): ModuleList(\n","          (0): TransformerEncoderLayer(\n","            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","            )\n","            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (1): TransformerEncoderLayer(\n","            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","            )\n","            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","          (2): TransformerEncoderLayer(\n","            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","            (attn): EfficientMultiheadAttention(\n","              (attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","              )\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (dropout_layer): DropPath()\n","            )\n","            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","            (ffn): MixFFN(\n","              (activate): GELU(approximate=none)\n","              (layers): Sequential(\n","                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n","                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n","                (2): GELU(approximate=none)\n","                (3): Dropout(p=0.0, inplace=False)\n","                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n","                (5): Dropout(p=0.0, inplace=False)\n","              )\n","              (dropout_layer): DropPath()\n","            )\n","          )\n","        )\n","        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  init_cfg={'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b5_20220624-658746d9.pth'}\n","  (decode_head): SegformerHead(\n","    input_transform=multiple_select, ignore_index=255, align_corners=False\n","    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n","    (conv_seg): Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))\n","    (dropout): Dropout2d(p=0.1, inplace=False)\n","    (convs): ModuleList(\n","      (0): ConvModule(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activate): ReLU(inplace=True)\n","      )\n","      (1): ConvModule(\n","        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activate): ReLU(inplace=True)\n","      )\n","      (2): ConvModule(\n","        (conv): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activate): ReLU(inplace=True)\n","      )\n","      (3): ConvModule(\n","        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activate): ReLU(inplace=True)\n","      )\n","    )\n","    (fusion_conv): ConvModule(\n","      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activate): ReLU(inplace=True)\n","    )\n","  )\n","  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",")\n","2022-08-29 01:49:45,152 - mmseg - INFO - Loaded 2511 images\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","2022-08-29 01:49:48,914 - mmseg - INFO - Loaded 280 images\n","2022-08-29 01:49:48,915 - mmseg - INFO - Start running, host: root@344117b35ce9, work_dir: /content/mmsegmentation/work_dirs/config2\n","2022-08-29 01:49:48,915 - mmseg - INFO - Hooks will be executed in the following order:\n","before_run:\n","(VERY_HIGH   ) PolyLrUpdaterHook                  \n","(NORMAL      ) CheckpointHook                     \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_train_epoch:\n","(VERY_HIGH   ) PolyLrUpdaterHook                  \n","(LOW         ) IterTimerHook                      \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_train_iter:\n","(VERY_HIGH   ) PolyLrUpdaterHook                  \n","(LOW         ) IterTimerHook                      \n","(LOW         ) EvalHook                           \n"," -------------------- \n","after_train_iter:\n","(ABOVE_NORMAL) OptimizerHook                      \n","(NORMAL      ) CheckpointHook                     \n","(LOW         ) IterTimerHook                      \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","after_train_epoch:\n","(NORMAL      ) CheckpointHook                     \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_val_epoch:\n","(LOW         ) IterTimerHook                      \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_val_iter:\n","(LOW         ) IterTimerHook                      \n"," -------------------- \n","after_val_iter:\n","(LOW         ) IterTimerHook                      \n"," -------------------- \n","after_val_epoch:\n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","after_run:\n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","2022-08-29 01:49:48,916 - mmseg - INFO - workflow: [('train', 1)], max: 160000 iters\n","2022-08-29 01:49:48,916 - mmseg - INFO - Checkpoints will be saved to /content/mmsegmentation/work_dirs/config2 by HardDiskBackend.\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","Traceback (most recent call last):\n","  File \"./tools/train.py\", line 242, in <module>\n","    main()\n","  File \"./tools/train.py\", line 238, in main\n","    meta=meta)\n","  File \"/content/mmsegmentation/mmseg/apis/train.py\", line 194, in train_segmentor\n","    runner.run(data_loaders, cfg.workflow)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmcv/runner/iter_based_runner.py\", line 144, in run\n","    iter_runner(iter_loaders[i], **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmcv/runner/iter_based_runner.py\", line 61, in train\n","    data_batch = next(data_loader)\n","  File \"/usr/local/lib/python3.7/dist-packages/mmcv/runner/iter_based_runner.py\", line 34, in __next__\n","    data = next(self.iter_loader)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1347, in _next_data\n","    return self._process_data(data)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1373, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_utils.py\", line 461, in reraise\n","    raise exception\n","cv2.error: Caught error in DataLoader worker process 0.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/content/mmsegmentation/mmseg/datasets/custom.py\", line 215, in __getitem__\n","    return self.prepare_train_img(idx)\n","  File \"/content/mmsegmentation/mmseg/datasets/custom.py\", line 232, in prepare_train_img\n","    return self.pipeline(results)\n","  File \"/content/mmsegmentation/mmseg/datasets/pipelines/compose.py\", line 41, in __call__\n","    data = t(data)\n","  File \"/content/mmsegmentation/mmseg/datasets/pipelines/pixelNthick.py\", line 16, in __call__\n","    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)\n","cv2.error: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n","> Overload resolution failed:\n",">  - src is not a numpy array, neither a scalar\n",">  - Expected Ptr<cv::UMat> for argument 'src'\n","\n","\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n","<class 'mmcv.parallel.data_container.DataContainer'>\n"]}]},{"cell_type":"code","source":["%cd /content/mmsegmentation"],"metadata":{"id":"gbDfxudYXTTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661736969061,"user_tz":-540,"elapsed":272,"user":{"displayName":"qwe qwe","userId":"08905623062927154995"}},"outputId":"81cf6b61-8880-432d-879e-4f91b47a8fc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/mmsegmentation\n"]}]},{"cell_type":"code","source":["import numpy as np\n","a = np.zeros(10)\n","type(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsjM5LsDPLcD","executionInfo":{"status":"ok","timestamp":1661737486085,"user_tz":-540,"elapsed":428,"user":{"displayName":"qwe qwe","userId":"08905623062927154995"}},"outputId":"925d6f1e-aa6b-451b-fa8c-5a174779b170"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":21}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train.ipynb","provenance":[],"authorship_tag":"ABX9TyPQlqH6d+f+FCTcMwynPLwP"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}